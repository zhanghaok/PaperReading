## 低资源场景

### 【综述】Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective

**摘要**

> 知识提取（Knowledge Extraction，KE）旨在从非结构化文本中提取结构化信息，它经常面临数据稀缺和新兴的不可见类型，即低资源场景。关于低资源KE的许多神经方法已经被广泛研究并取得了令人印象深刻的性能。在本文中，我们对低资源情景下的KE进行了文献综述，并将现有的工作系统地分为三种范式：（1）开发高资源数据，（2）开发跨资源模型，以及（3）开发数据和模型。此外，我们描述了有前景的应用，并概述了未来研究的一些潜在方向。我们希望我们的调查能够帮助学术界和工业界更好地了解这个领域，激发更多的想法，推动更广泛的应用。

**问题引出**

> 广泛使用的KE任务包括命名实体识别（NER）[Chiu and Nichols，2016]、关系提取（RE）[Zenget al.，2015]和事件提取（EE）[Chenet al.，2015]目标是从非结构化文本中提取结构信息，但在实践中容易出现标记数据，并且由于域、语言和任务的变化而面临无法识别的挑战。这些缺点导致了低资源KE的研究，它可以通过访问少量样本来提高机器智能和现实应用的学习能力。

**低资源场景**

大多数传统的KE模型都假设，要获得令人满意的性能，必须有足够的训练语料库。然而，在实际应用中，特定于任务的标记样本往往分布不均，新的未标记类可能会迅速演化，从而导致资源不足的情况。考虑到样本的不均匀分布和新的看不见的类，我们系统地将低资源场景分为三个方面。

- Long-tail Scenario：长尾场景意味着只有极小的类是数据丰富的，而大多数类的标记样本数据非常差。

- Few-shot Scenario：少样本场景意味着候选类只有少量的标记样本，其中少量样本可以按照n-way-k-shot设置或相对于总样本的未执行情况进行混合。

- Zero-shot Scenario：零样本场景意味着，用于预测的测试样本从未出现在训练样本中。

**解决方案**

> 根据解决低资源KE问题时增强的方面，我们将现有方法分为三种通用范例，如表1所示。为了适应低资源场景，模型倾向于（1）利用高资源数据；（2） 开发StrongerModels；（3）利用数据&模型获取

![image-20220422191444788](D:\我的GitHub仓库\PaperReading\imgs\LowS)

**1 利用高资源数据**

> 这种范式是指借助于内源性生成或外源性导入的辅助资源，对原始小样本数据进行数据扩充或知识增强。更高的资源数据有望产生更丰富、更干净的样本，以及更精确的语义表示。

弱监督增强是指通过弱/远程监督自动生成训练数据，旨在调整更多数据。这种方法利用一个知识库和一些启发式规则来自动重新标记企业中的状态。当然，生成的训练集可能包含很多噪声，这就是此类方法受到弱监督的原因。

- 多模态增强是指导入多模态样本，以补充文本中缺失的语义，帮助精确提取知识。直观地说，这种方法的主要挑战在于多模态的融合。

- 多语言增强是指导入多语言样本，与多模态增强类似，旨在获得更丰富、更稳健的样本表示。直觉上，这种方法的主要挑战是获得跨语言表达.

- 辅助知识增强意味着利用外部知识作为补救措施，旨在更准确地学习样本的语义表示（包括文本、知识图谱、本体&规则）。

**2 开发更强的模型**

> 这种范式指的是开发更健壮的模型，以更好地应对样本的不均匀分布和新的看不见的类。更强的模型有望提高学习能力，从而充分利用现有的小数据，减少对样本的依赖

- **元学习**通过从少量实例中学习，迅速吸收新知识，推出新课程，具有“学会学习”的能力，自然适合于少量任务。对于少样本NER，[Liet al.，2020a]实施了典型网络[Snellet al.，2017]，这是一种度量学习技术。它学习单词的中间表示，这些单词可以很好地聚集到命名的实体类中，使得它能够用极其有限的训练样本对单词进行分类，并且可以潜在地用作零样本学习方法。[Hanet al.，2018]将RE（尤其是长尾关系）表述为少样本任务，[Denget al.，2020]将有限标记数据的EE任务重新表述为几次学习问题。对于少样本RE而言，[Gao等人，2019a]为原型网络配备了混合注意，该注意由实例级注意和特征级注意组成，分别用于选择信息更丰富的实例和突出特征空间中的重要维度。[Quet al.，2020]提出了一种新的贝叶斯元学习方法，可以有效地学习关系原型向量的后验分布。关键思想是考虑全局关系图，它捕获关系之间的全局关系。

- **迁移学习**通过转移学习到的表示和模型，特别是从高到低的资源，减少了对标记目标数据的需求。在这个调查中，我们考虑基于类相关语义和预训练语言表示的迁移学习方法。（1） 类相关语义；（2） 预先训练的语言表达：基于预先训练的语言表示的迁移学习使用预先训练的语言表示，这些语言表示基于未标记的数据（如BERT）进行训练，并加载预先训练的参数进行微调。

- **提示学习**通常包括一个要求模型做出特定预测的模式和一个将预测转换为类标签的描述器。即时学习的核心思想是在输入中插入文本片段，即模板，并将分类任务转化为一个隐藏的语言建模问题。(1) Vanilla Prompt Learning (2)(2) Augmented Prompt Learning.

**3 同时利用数据和模型**

> 这种范式指的是联合使用代表性样本和稳健模型，使其易于迅速适应低资源场景。利用数据&个模型结合起来，有望在现有稀疏数据的情况下寻找更合适的学习策略。

- **多任务学习**通过联合利用任务通用共性和任务特定差异，同时学习多个相关任务，从而提高任务特定模型的学习效率和预测精度，这自然能够改善低资源场景中的目标任务。[Zenget al.，2020]研究了联合NER和RE任务，并提出了一个配备复制机制的多任务学习框架，允许模型预测多令牌实体。此外，该多任务学习框架能够区分RE任务中定义的头实体和尾实体，有助于准确提取实体。[Genget al.，2021]研究了联合NER和RE任务，提出了一种基于卷积神经网络和递归神经网络相结合的注意机制的端到端方法，该方法可以获得丰富的语义，并充分利用实体和关系之间的关联信息，而不引入外部复杂特征。

- 将KE描述为QA/MRC将KE任务描述为**问答（QA）/机器阅读理解（MRC）问题**，这意味着实体、关系和事件的提取转化为从上下文中识别答案的任务。事实上，问题查询可以暗示目标任务的关键知识。

- **检索增强**将检索集成到预训练语言模型中，因此，通过简单地更新用于检索的数据，模型可以更高效地使用参数，并有效地实现域自适应。



**一些基准数据集**

![image-20220422192807619](D:\我的GitHub仓库\PaperReading\imgs\LSDS)

### Template-Based Named Entity Recognition Using BART (ACL2021)

#### 研究背景

> 神经网络NER需要大量的标签数据，在有些特定领域可能会有大量标签数据，但大部分领域都稀缺。理想情况，从源域转移知识是可行的。然而实际上不用域之间的实体类别是不同的。softmax 层和 CRF层要求训练和测试之间的标签集一致。因此，给定一个新的目标域，输出层需要调整，训练必须再次使用源和目标进行域，这可能是昂贵的。
>
> 转向基于度量的小样本NER研究。主要想法是基于源域中的实例训练一个相似函数，然后在目标域中使用这个相似函数作为最近邻准则。这种方法极大地降低了域适应的成本，特别是在目标域数量较大的情况下。但是有两个缺点：①目标域的标记实例被用来寻找启发式近邻算法的最佳超参数设置而不是更新NER模型中的参数。②过度依赖源域和目标域之间的相似文本模式。

#### 动机

- 现有的few-shot NER使用基于相似度的度量,不能充分利用NER模型参数中的知识迁移
- 基于微调的NER模型,进行领域迁移时,需要对输出层进行调整,必须使用源域和目标域再次训练,代价昂贵
- 目标领域的实体集合可能和源领域有很大差别,例如,新闻领域里提及一个人，类别往往是‘PERSON’，但是在电影领域，他的类别可能就是“CHARACTER”

提出一种基于模板的NER方法,将NER任务视为一个在seq2seq框架下的语言模型排名问题。

#### 背景知识

**少样本NER**

![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-1.png)

低资NER的标签集合可能不同于富有NER的。

**传统序列标注**

![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-2.png)





但是因为标签集不同，所以输出层的参数不同。我们需要在新的域从新训练输出层的参数。然而，这种方法确实没有充分利用标签关联（例如，“人”和“角色”之间的关联），也不能直接用于零样本情况，即目标域中没有可用的标签数据。

#### 本文提出的基于模板的方法



![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-3.png)

**人工模板构造**

正例和负例，1：1.5。不同模板具有不同的效果。

![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-5.png)

**模型推理**

我们首先列举句子{x1，…，xn}中所有可能的跨度，并将它们填入准备好的模板中。为了提高效率，我们将n-gram的数量限制在1到8之间，因此为每个句子创建8n个模板。然后，我们使用经过微调的预训练生成语言模型为每个模板Tyk分配一个分数

![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-4.png)

**训练过程**

喂给编码器的是原始的句子。第c步解码器的输入包括编码器的输出和上一时刻解码器的输出。第c步解码器的输出的词的概率为：

<img src="D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-6.png" style="zoom:33%;" />

其中|v|表示预训练词汇表的大小。解码器输出和原始模板之间作交叉熵损失

<img src="D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-7.png" style="zoom:33%;" />

**迁移学习**

给定一个少样本实例的新领域P,标签集为$ L^P$ (可能包含与已经训练的NER模型不同的实体类型)

- 用新的标签集填充模板,得到少量的的 $\left(X^{P}, T^{P}\right)$ 序列对，模型其他部分不动。
- 对在富样本下训练的NER模型（也就是在CoNLL03上训练好的）进行微调。

输出的是一个自然句子,而不是一个标签,不管是富资源数据集,还是低资源数据集,标签都是在预训练模型中的子集。

模型学到了很多相关性信息,比如原来学习过实体类型的city,那么少样本情况下,有一个实体类型location,city的相关性,能够提升跨领域学习效果。

#### 实验结果

**数据集**

- CoNLL03作为富资源NER数据集

- MIT Movie Review,MIT Restaurant Review,ATIS作为少样本跨领域的数据集。

**CoNLL03数据集结果**

![img](https://img-blog.csdnimg.cn/img_convert/effd48162ed0ae5051bfa3544036c309.png)

- 基于序列标注的BERT模型给出了一个起点很高的基线
- 即使基于模板的BART模型是被设计用作少样本NER,但是,在富样本设置下,表现得同样有竞争力…其中,召回率比基于序列标注的BERT模型高1.8%这表明这个方法确实能够提高识别实体的能力
- 通过对比基于序列标注的BART模型和基于模板的BART模型,可以发现,虽然都是使用BART Decoder,但是基于模板的方法效果明显更好,证明了基于模板方法的有效性
- BART序列分类做的不好可能原因是基于seq2seq的denoising autoencoder training

**领域内少样本NER**

1. 设定"MISC"和"ORG"为富样本实体,"LOC"和"PER"为少样本实体
2. 从CONNL03中下采样3806个训练样本,其中包含3925个"ORG",1423个"MMISRC",50个"LOC",50个"PER"

![image-20220311200135042](https://img-blog.csdnimg.cn/img_convert/8f6d942f653307c9608a7e8b19fa98e6.png)

- 富样本实体类别识别中,和基于序列标注的BERT表现得一样好
- 少样本实体类别识别中,F1 分数显著的领先于BERT模型 , "LOC"和"MISC"分别高11.26,12.98

**跨领域少样本NER**

1. 从大的训练集中随机抽取训练实例作为目标域的训练数据,其中每个实体类型随机抽取固定数量的实例
2. 使用不同数量的实例进行训练

![](D:\我的GitHub仓库\PaperReading\imgs\2022-5-5-.png)

#### 结论

本文以BART为骨干网络,研究基于模板的少样本NER

- 当出现新的实体类别时,可以直接针对目标领域进行微调,这比基于序列标注的NER模型更强大
- 在富资源任务上,取得了有竞争力的结果;在低资源跨领域NER任务上,明显优于序列标注和基于距离的方法

### LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER(AAAI2022)

> 本文对模型（代码的改进）在复旦邱锡鹏团队“*A Unified Generative Framework for Various NER Subtasks*”
>
> 本文对前人的工作的改进是基于~“*Template-Based Named Entity Recognition Using BART*”的缺点进行创新的。

**摘要**

大多数现有的NER方法依赖于大量有标签的数据，这在训练数据有限的低资源场景中很难实现。最近，针对预训练语言模型的“提示+微调”的方法，利用提示作为任务知道来减少预训练任务和下游任务调整之间的gap，在少样本场景下取得了显著的效果。受到提示学习的启发，我们提出一种新的轻量级的生成框架，该框架具有提示引导的注意力机制用于低资源NER。具体而言，我们构建了语义感知的实体类别答案空间，以便在没有任何特定分类器的情况下快速生成实体span和实体类别。我们进一步提出了提示引导注意力机制，将连续的提示纳入到自注意力层，以便重新调节预训练的权重。请注意，我们只在预训练语言模型的整个参数固定的情况下调整这些连续提示，因此，我们的方法对于低资源场景来说是轻量级和灵活的，并且可以更好地跨域传递知识。实验结果表明，在标准监督设置下，LightNER可以获得相当的性能，在低资源设置下，只需调整一小部分参数，其性能就会优于强基线 。

**前言**

- BERT-分类器的方法不适用于不可见类，因此需要重新训练。
- 原型网络方法减少了域适应的代价，然而这些方法主要侧重寻找最佳的超参数设置，以利用源域和目标域之间的相似模式，而不是更新NER的网络参数。
- 基于提示学习的方法想去弥补下游任务和预训练语言模型之间的gap。
- 直观的说：提示学习方法适应于小样本NER！
- “*Template-Based Named Entity Recognition Using BART*”是第一个针对少样本NER提出基于BART构建模板的方法。但是这篇文章有2个不足之处：（1）手工构建的模板比较敏感。（2）计算复杂度高。需要枚举n-gram去填充构建的m个模板。
- 因此针对上面2个不足提出了本文的idea（做成改进）。具体而言改进如下：（1）也是基于BART的seq2seq模型（这点不算改进，但是是优点可以解决不可见类的问题，不用重新构建特定的分类器）；（2）连续的提示表征向量可以学习，而不是手工构建的模板；（3）轻量级（计算复杂度低），不更新预训练语言模型的参数，只更新连续提示向量和答案空间的权重。

**方法**

![image-20220526100057456](D:\我的GitHub仓库\PaperReading\imgs\LightNER.png)

- [x] 任务构建

输入序列：$$ X={x_1,x_2,...,x_n}.$$NER任务旨在提供实体范围的开始索引和结束索引，以及实体类型，在我们的框架中分别由e、t表示。其中e是token的索引，$t \in\{$ person, organization，..., $\}$。在我们的生成框架中，目标序列Y由多基预测![image-20220526105103898](D:\我的GitHub仓库\PaperReading\imgs\light-for4.png)组成。 

给定令牌序列X，条件概率计算如下：
$$
P(Y \mid X)=\prod_{t=1}^{n} p\left(y_{t} \mid X, y_{0}, y_{1}, \ldots, y_{t-1}\right)
$$

- [x] 生成框架

过程和Template-Based Named Entity Recognition Using BART类似，参考之即可。

![image-20220526105009135](D:\我的GitHub仓库\PaperReading\imgs\light-for2.png)

- [x] 为提示学习构建语义感知的答案空间

**Q**:为何要构建？前人的基于特定类别的有何缺点？

**A**:现有研究（Liu等人，2021c；Le Scao和Rush，2021）表明，answer工程对“提示-微调”的性能有很大影响。对于NER中实体类别的预测，添加表示不同实体类型的额外*标签特定*的参数将阻碍快速学习的适用性，并损害低资源NER中类间的知识转移。同时，手动在词汇表中找到合适的标记来区分不同的实体类型也是一个挑战。 此外，某些实体类型在特定的目标域中可能很长或很复杂，例如return_date。TIS中的month_name和MIT restaurant中的restaurant_name。 **为了解决上述问题，我们构建了包含与每个实体类相关的多个标签词的语义感知的答案空间**，并利用加权平均方法答案空间V。具体而言，我们定义了从实体类别C的标签空间到语义感知答案空间V的映射M，即M:C->V![image-20220526105219767](D:\我的GitHub仓库\PaperReading\imgs\light-for5.png)我们使用$V_c$表示V的子集，该子集由特定的实体类型c映射，$\cup_{c \in \mathcal{C}} \mathcal{V}_{c}=\mathcal{V}$。以上述c1=“return_date.month_name”为例，我们根据c1的分解定义Vc1={“return”、“date”、“month”、“name”)。由于直接平均函数可能存在偏差，我们采用可学习权重α对答案空间中标签词的logit进行平均，作为预测logit :

![image-20220526104934761](D:\我的GitHub仓库\PaperReading\imgs\light-for1.png)

$\alpha_{v}{ }^{c}$表示实体类型c的权重。<font color="#6660000">**通过构建语义感知的答案空间，LightNER可以在不修改PLM的情况下感知实体类别中的语义知识**</font>。 

- [x] 提示引导的注意力机制

“参数设置”

具体而言，LightNER分别为编码器和解码器添加两组可训练的嵌入矩阵$\left\{\phi^{1}, \phi^{2}, \ldots, \phi^{N}\right\}$，并将transformer层数设置为N，其中$\phi_{\theta} \in \mathbb{R}^{2 \times|P| \times d}$（由θ参数化），| P |是提示的长度，d代表dim(ht)，2表示φ是为Key和Value设计的。在我们的方法中，LM参数是固定的，提示参数θ和α的可学习分布是唯一可训练的参数。 

“提示引导的注意力层”

![image-20220526100443183](D:\我的GitHub仓库\PaperReading\imgs\Prompt-attention.png)

LightNER继承了transformer的架构，它是一堆相同的构建块，用前馈网络、剩余连接和层规范化包裹起来。作为一个特定组件，我们在原始query\key\value层上引入了提示引导注意层，以实现灵活有效的提示调整。 

给定一个输入标记序列X={x1，x2，…，xn}，按照上述公式，我们可以将提示的表示与自我注意的计算合并到X中。在每个层l中，输入序列表示$X^l∈ R^d$首先投影到query\key\value向量中 :

![image-20220526105259174](D:\我的GitHub仓库\PaperReading\imgs\light-for6.png)


所提出的提示引导注意可以根据提示词重新调节注意的分布。因此，该模型得益于提示的指导 .

**实验结果**

> 实验设置和*Template-Based Named Entity Recognition Using BART*的实验设置是基本保持一致的。

- [x] 标准的监督场景NER（CoNLL03数据集）

![image-20220526100950674](D:\我的GitHub仓库\PaperReading\imgs\light-table1.png)

- [x] 域间少样本场景NER（CoNLL03数据集）

![image-20220526101059185](D:\我的GitHub仓库\PaperReading\imgs\light-table2.png)

MISC指标有误。

- [x] 跨域少样本场景NER（MIT Movie、MIT Restaurant、ATIS）

![image-20220526101242732](D:\我的GitHub仓库\PaperReading\imgs\light-table3.png)

- [x] 消融实验

![image-20220526101436848](D:\我的GitHub仓库\PaperReading\imgs\light-table4.png)


- [x] 零样本场景

![image-20220526101504225](D:\我的GitHub仓库\PaperReading\imgs\light-table5.png)

**结论和未来工作**

在本文中，我们提出了一种新的具有即时引导注意的生成框架（LightNER），它可以用很少的样本来识别看不见的实体。通过为PrompTuning构建实体类型的语义感知答案空间，LightNER可以保持一致的预训练和微调过程。同时，提示引导注意的设计可以更好地跨领域传递知识。我们的模型在参数方面是有效的，只需调整提示参数。实验结果表明，LightNER在资源丰富的情况下可以获得有竞争力的结果，在资源较少的情况下优于基线方法。<font color="#6660000">**未来，我们计划探索更复杂的方法来增强提示，并将我们的方法应用于低资源环境中的更多任务.**</font>。

### InstructionNER： A multi-task instruction-based generative framework for few-shot NER(Arxiv 2022)

> 试图激发预训练语言模型潜力的方法有2种：①是基于提示学习的方法；②是基于指令的方法

**摘要**

> 最近基于提示学习的方法在少样本场景下取得了很好的效果，主要是通过消除语言模型的预训练和下游任务的微调之间的gap。<font color="#6660000">**然而，现有的提示需欸模板大多数是为句子级别任务设计的，不适于序列标注目标**。</font>为了解决上述问题，我们提出了多任务基于指令的生成式框架，InstructionNER。具体来说，我们将NER任务重新表述为一个生成问题，该问题使用特定于任务的指令和答案选项丰富源语句，然后推断自然语言中的实体和类型。 我们进一步提出了两个辅助任务，包括实体提取(entity extraction)和实体类型(entity typing)，分别使模型能够捕获更多实体的边界信息和加深对实体类型语义的理解。实验结果表明，在五个数据集上，我们的方法在很少的镜头设置下始终优于其他基线。 

**解决NER的四种范式**

![image-20220529094115153](./imgs/InsNER-1.png)

序列标注：不幸的是，在现实世界中，用于标记的注释资源往往稀缺且昂贵。

基于模板：尽管基于提示的学习方法在数据稀缺的情况下取得了优异的性能 。然而现有的大多数基于模板的方法都是为句子级别任务设计的，很难适应序列标注问题。因此，需要将传统的NER范式重新制定为更适合PLMs的方式，以便将快速学习方法应用于NER任务。 最近Cui提出的<font color="#6660000">**Template-based**</font>（图1b）将NER问题转换为完形填空的形式。虽然由于引入了提示，该方法在少样本场景中的性能优于传统的序列标记基线，但它需要枚举所有跨候选序列，这既不美观又耗时。 

BART-NER的：（复旦邱锡鹏团队）BART-NER（如图1c所示）提出了一种基于指针的seq2seq体系结构，该体系结构将NER子任务转换为统一的序列生成任务，并从输入句子和相应的类型索引中预测实体 。<font color="#6660000">**LightNER**</font>（Chen等人，2021）对<font color="#6660000">**BART-NER**</font>的注意机制进行了快速调整，并在低资源场景中取得了有希望的改进。 

基于指令的（本文的方法）：受TemplatedNER（设计特定提示模板作为解码器输入）和LightNER（将额外参数作为软提示引入注意层）的启发，我们提出了这样一个问题：<font color="#6660000">**如果我们用启发式提示丰富生成性PLM的源语句，这能更好地激发在预训练阶段学习的语义知识并完成低资源的NER任务吗？** </font>

这就是本文作者Idea的产生，为了解决这样的问题，作者有啥如何处理的?

> 为此，我们提出了一个基于多任务指令的生成框架，名为InstructionNER，用于解决少样本NER。具体来说，我们将NER任务重新表述为一个自然语言生成问题（如图1d所示）。对于源语句，我们设计了描述性说明，使模型能够理解不同的任务（Wei等人，2021），并采用了一种选项机制，包括所有候选实体类别作为输出空间的约束。 然后，为了进行推理，需要T5以自然语言的形式生成实体词和相应的类型，因为我们认为，**无限制的解码**将激发PLM的潜在知识，从而在更大程度上完成实体提取和识别任务。此外，我们还介绍了两个辅助任务，命名实体提取（EE）任务和实体类型化（ET）任务。 EE要求模型只解码实体名称，并学习更好地捕获边界信息。ET旨在预测实体类型并增强PLM对类型语义的理解 

本文贡献总结为：

- 1） 为了充分利用语言模型中的知识，我们将NER任务重新表述为一个新的seq2seq问题，该问题将描述性任务说明和答案选项集成到源语句中，然后要求模型预测自然语言中的实体名称和类型。
- 2） 此外，我们还提出了两个辅助任务，可以增强捕获实体边界的能力和对类型语义的理解。
- 3） 在三个NER基准上的实验表明了我们提出的方法的有效性，尤其是在数据稀缺的情况下。此外，我们还进行了深入的分析，以展示我们的方法的更多特点。 

**相关工作**

简述一下

- NER任务：最流行的表述为序列标注任务，在句子编码器后接一个分类器或者CRF。但是仅仅适用于平滑的NER，不适合嵌套场景。因此复旦邱团队受预先训练的seq2seq模型最近成功的启发提出了BART-NER，将所有三种NER子任务重新表述为生成问题，并使用BART为推理实体及其类型索引建立了基于指针的框架。<font color="#6660000">**受这种新范式的启发，我们将NER视为自然语言生成任务，其中模型需要以自然语言的形式生成实体名称和相应的类型。此外，我们采用T5代替BART作为基础模型，因为T5的预训练任务是预测损坏令牌的序列，这更适合我们的公式。** </font>
- 提示学习：动机是充分利用预训练语言模型学习到的知识。另外有一种激发预训练语言模型潜能的方法是基于指令的。我们结合两者提出模板指令。
- 基于提示的少样本NER：Template-Based、LightNER。

**本文的方法**

- [x] 问题的定义

输入序列：![image-20220529100214453](./imgs/InsNER-2.png)

识别：![image-20220529100328141](./imgs/image-20220529100328141.png),l表示左边界，r表示右边界，t表示实体类型。

为简单起见，我们使用xl:r表示x从左边界l到右边界r（包括）的跨度，即xl:r={wl，…，wr}。

- [x] 通过T5解决NER

为了更好地转移和利用在预先训练的语言模型中学习到的知识，我们将NER任务重新格式化为seq2seq形式，并通过微调T5解决它，如图2所示 。

![image-20220529100556745](./imgs/image-20220529100556745.png)

具体而言，对于**主任务**（如图2中的橙色方框所示），每个输入由以下三个字段组成： 

- Sentence:原始句子
- Instruction：该指令告诉模型当前示例所属的任务。对模型进行训练，以生成与指令一致的预期输出。对于主NER任务，指令为：<font color="#6660000">***please extract entities and their types from the input sentence, all entity types are in options***</font>.
- 选项：所有实体类型T，用逗号分隔。此字段同时充当提示和约束，以提醒模型需要识别哪些实体类型。

为了激发预训练模型的潜力，我们将输出组织成自然语言形式，自然地响应输入命令。具体来说，对于实体出现（l、r、t），我们使用模板将其转换为自然语言形式：xl:r 是a/an t，并将所有转换的实体出现连接在一起，组成一个输出句子（使用逗号作为分隔符，以点结尾）。

**辅助任务**为了在更细粒度的级别上提高性能，我们进一步设计了两个辅助任务，即实体提取和实体类型，这两个任务正是构成整个NER任务的两个细粒度子任务。我们与这些辅助任务一起训练模型。 

对于**实体提取任务**，训练模型从给定句子中提取实体跨度，但不需要键入它们。我们将指令字段替换为<font color="#6660000">***please extract entity words from the input sentence***</font> ，并删除选项字段，因为不需要键入提取的跨度。此外，输出应仅包含提取的跨距，并删除“is a /an t”。实体提取任务是主任务的简化。该模型在指令的指导下，只需从句子中提取实体，而不需要关注所提取实体的类别信息，这对提高实体提取的广度F1有很大帮助。span F1的改进意味着增强了从句子中正确提取实体的能力，从而提高了NER主任务的准确性。 

对于实体类型任务，将对模型进行训练，以type句子中给定的实体。具体来说，我们将输入示例的指令字段替换为<font color="#6660000">***please typing these entity words according to the sentence: 《the given entity occurences》***</font>，其他字段和输出与主任务相同。在训练阶段，随着句子中实体出现次数的给定，我们期望该模型更多地关注实体出现的类别语义信息。这鼓励模型生成更准确的类别标签，同时保持在主NER任务中生成实体引用的准确性，从而提高NER的性能。

**实验部分**

![image-20220529101543572](./imgs/image-20220529101543572.png)

![image-20220529101606462](./imgs/image-20220529101606462.png)

![image-20220529101628968](./imgs/image-20220529101628968.png)